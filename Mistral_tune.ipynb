{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMZogGUBs0lhcBvL9bovS5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sml8648/BERT-pytorch/blob/master/Mistral_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI-C69CqKXZy"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "kzy6ILkJKaJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "id": "TMUEbRR-KcR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
      ],
      "metadata": {
        "id": "-xKE8Ta6KcZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl xformers datasets einops sentencepiece"
      ],
      "metadata": {
        "id": "CE3zbaRzKdcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "import os, torch, platform, warnings\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "KbKgc5yfKeYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use a sharded model to fine-tune in the free version of Google Colab.\n",
        "base_model = \"mistralai/Mistral-7B-v0.1\" #bn22/Mistral-7B-Instruct-v0.1-sharded\n",
        "dataset_name, new_model = \"gathnex/Gath_baize\", \"gathnex/Gath_mistral_7\""
      ],
      "metadata": {
        "id": "mAmZxnIlKvoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a Gath_baize dataset\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "dataset[\"chat_sample\"][0]"
      ],
      "metadata": {
        "id": "NuqODaBYYgkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model(Mistral 7B)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")"
      ],
      "metadata": {
        "id": "KTENjbYTLtgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "id": "7BP1fJ-0Lw4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "_crN04DDLySt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "bj_1V5WJL6ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "IGtX1ZsTXbA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.add_bos_token, tokenizer.add_eos_token"
      ],
      "metadata": {
        "id": "NTJkkPqSXcSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=8,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
        "    )\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "XjQMasDSYSh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Arguments\n",
        "# Hyperparameters should beadjusted based on the hardware you using\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir= \"./results\",\n",
        "    num_train_epochs= 1,\n",
        "    per_device_train_batch_size= 1,\n",
        "    gradient_accumulation_steps= 2,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    save_steps= 5000,\n",
        "    logging_steps= 30,\n",
        "    learning_rate= 2e-4,\n",
        "    weight_decay= 0.001,\n",
        "    fp16= False,\n",
        "    bf16= False,\n",
        "    max_grad_norm= 0.3,\n",
        "    max_steps= -1,\n",
        "    warmup_ratio= 0.3,\n",
        "    group_by_length= True,\n",
        "    lr_scheduler_type= \"constant\",\n",
        ")"
      ],
      "metadata": {
        "id": "kD-drJnsYXWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting sft parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length= None,\n",
        "    dataset_text_field=\"chat_sample\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing= False,\n",
        ")"
      ],
      "metadata": {
        "id": "Hps6HZC_YayV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "l_BzFnBVYcMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yqDxXzP_Zb6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_2uB0oL6ed7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PeBIwRDlwOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}